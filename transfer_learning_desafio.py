# -*- coding: utf-8 -*-
"""Transfer_Learning_Desafio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10_hjK3Zg5rM0KBwyl6le7fNE1UzvLuFV
"""

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
import matplotlib.pyplot as plt
import os
import zipfile

# Load the Dataset

dataset_path = "/content/liquidificador_e_batedeira.zip"
extract_path = "/content/liquidificador_e_batedeira_filtered"

with zipfile.ZipFile(dataset_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

data_dir = extract_path
print("Dataset extracted to:", data_dir)

# Preprocessing the data
train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(160, 160),
    batch_size=32)

validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(160, 160),
    batch_size=32)

# Load the pre-trained model
pretrained_model = tf.keras.applications.MobileNetV2(input_shape=(160, 160, 3),
                                                      include_top=False,
                                                      weights='imagenet')

# Freeze the layers of the base model
pretrained_model.trainable = False

# Create the final model
model = models.Sequential([
    pretrained_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(2, activation='softmax')  # For two classes: "liquidificador" and "batedeira"
])

# Compile the model
model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=validation_dataset)

# Evaluate the model
loss, accuracy = model.evaluate(validation_dataset)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Visualize the results
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Test with real images
from google.colab import files
from tensorflow.keras.preprocessing import image
import numpy as np

uploaded = files.upload()

for fn in uploaded.keys():
    img_path = fn
    img = image.load_img(img_path, target_size=(160, 160))
    img_array = image.img_to_array(img) / 255.0  # Normalizando a imagem
    img_array = tf.expand_dims(img_array, axis=0)  # Adicionando a dimensão de batch

    predictions = model.predict(img_array)  # Realizando a predição
    predicted_class_index = np.argmax(predictions)  # Pegando o índice da classe com maior probabilidade

    # Definindo as classes
    class_names = ['Batedeira', 'Liquidificador']  # Ordem de classes que você usou no treinamento
    predicted_class = class_names[predicted_class_index]  # Nome da classe com maior probabilidade

    print(f"The image is a: {predicted_class}, with probabilities: {predictions}")